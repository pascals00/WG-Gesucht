{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WG - Gesucht Data Analysis \n",
    "#### Step 1: Data Cleaning \n",
    "\n",
    "- Data cleaning involves filtering out premium status ads and removing duplicates. \n",
    "- Duplicates are identified by the combination of title, address, and duplicate IDs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The length of the entire dataset is: 16120\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Get the parent directory of the current working directory\n",
    "parent_dir = os.path.abspath(os.path.join(os.getcwd(), os.pardir))\n",
    "\n",
    "apartmentsDataPath = os.path.join(parent_dir, 'WebCrawlerApp/data/output/apartmentsBerlinData.csv')\n",
    "\n",
    "# Read the CSV file\n",
    "df = pd.read_csv(apartmentsDataPath, usecols=list(range(0, 45)))\n",
    "\n",
    "print(f\"The length of the entire dataset is: {len(df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "%run './NotebookSetup/Style.ipynb'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 1.1: Filter Premium Ads from the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The size of the dataset reduced by Premium ads is: 10233 \n",
      "\n",
      "There are 5887 Premium ads in the dataset.\n"
     ]
    }
   ],
   "source": [
    "df_without_premium = df[df['premiumstatus'] == False]\n",
    "\n",
    "print(f\"The size of the dataset reduced by Premium ads is: {len(df_without_premium)} \\n\\nThere are {len(df) - len(df_without_premium)} Premium ads in the dataset.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 1.2: Filter duplicate IDs from the WG-Gesucht Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Overwrite the original DataFrame with the one without Premium ads\n",
    "df = df_without_premium\n",
    "\n",
    "def compare_duplicates (column_name: str): \n",
    "    # Step 1: Find duplicates based on apartmentID\n",
    "    dup_ids = df[df.duplicated(subset=[column_name], keep=False)][column_name].unique()\n",
    "\n",
    "    # Prepare a new DataFrame for the comparison\n",
    "    comparison_columns = ['apartmentID1', 'apartmentID2', 'same_id', 'title1', 'title2', 'same_title', 'address1', 'address2', 'same_address', 'street1', 'street2', 'same_street', 'zip1', 'zip2', 'same_zip', 'city1', 'city2', 'same_city']\n",
    "    df_comparison = pd.DataFrame(columns=comparison_columns)\n",
    "\n",
    "    # Step 2: Find duplicates based on ID\n",
    "    for apt_id in dup_ids:\n",
    "        # Filter the DataFrame for the current apartmentID\n",
    "        dup_df = df[df[column_name] == apt_id]\n",
    "\n",
    "        # Ensure there are at least two entries for comparison\n",
    "        if len(dup_df) < 2:\n",
    "            print(dup_df)\n",
    "            continue  \n",
    "        \n",
    "        # Choose the first two entries for comparison\n",
    "        first, second = dup_df.iloc[0], dup_df.iloc[1]\n",
    "        \n",
    "        # Create a new row for the comparison DataFrame\n",
    "        new_row = {\n",
    "            'apartmentID1': first['apartmentID'], \n",
    "            'apartmentID2': second['apartmentID'], \n",
    "            'same_id': True,  # Da sie dieselbe apartmentID haben\n",
    "            'title1': first['title'], \n",
    "            'title2': second['title'], \n",
    "            'same_title': first['title'] == second['title'], \n",
    "            'address1': first['address'], \n",
    "            'address2': second['address'], \n",
    "            'same_address': first['address'] == second['address'], \n",
    "            'street1': first['street'],\n",
    "            'street2': second['street'],\n",
    "            'same_street': first['street'] == second['street'],\n",
    "            'zip1': first['postcode'],\n",
    "            'zip2': second['postcode'],\n",
    "            'same_zip': first['postcode'] == second['postcode'],\n",
    "            'city1': first['city'],\n",
    "            'city2': second['city'],\n",
    "            'same_city': first['city'] == second['city']\n",
    "        }\n",
    "        df_comparison = pd.concat([df_comparison, pd.DataFrame([new_row], columns=comparison_columns)])\n",
    "\n",
    "    print(f\"Number of duplicates based on {column_name}: {len(df_comparison)}\")\n",
    "    return df_comparison\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of duplicates based on apartmentID: 1358\n"
     ]
    }
   ],
   "source": [
    "df_comparison = compare_duplicates('apartmentID')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1.2.1: View Same IDs and address with different titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "229 duplicates based on apartmentID, that have the same address but different titles.\n",
      "\n",
      "When you view the table, you can see that the user or owner of the add changed the title over time. \n",
      "\n",
      "Result: Duplicates based on ID and Address with false Title will be removed from the dataset. \n"
     ]
    }
   ],
   "source": [
    "df_FalseTitle = df_comparison[(df_comparison['same_id'] == True) & (df_comparison['same_title'] == False) & (df_comparison['same_address'] == True)]\n",
    "\n",
    "print(f\"\"\"\n",
    "{len(df_FalseTitle)} duplicates based on apartmentID, that have the same address but different titles.\n",
    "\n",
    "When you view the table, you can see that the user or owner of the add changed the title over time. \n",
    "\n",
    "Result: Duplicates based on ID and Address with false Title will be removed from the dataset. \"\"\")\n",
    "\n",
    "#display_as_table(df_FalseTitle) # Here you can output the entire table of the False titles with duplicate ids and addresses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1.2.2: View Same IDs and Title with different address. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "18 duplicates based on apartmentID, that have the same titles but with different addresses.\n"
     ]
    }
   ],
   "source": [
    "df_FalseAddress = df_comparison[(df_comparison['same_id'] == True) & (df_comparison['same_title'] == True) & (df_comparison['same_address'] == False) ]\n",
    "\n",
    "print(f\"\"\"\n",
    "{len(df_FalseAddress)} duplicates based on apartmentID, that have the same titles but with different addresses.\"\"\")\n",
    "\n",
    "#display_as_table(df_falseadressAndTitle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Levenshtein import ratio\n",
    "\n",
    "def compare_streetnames(df_differentstreets):\n",
    "    data_List = []\n",
    "\n",
    "    for index, row in df_differentstreets.iterrows():\n",
    "        data = {'street1' : row['street1'], 'street2' : row['street2']}\n",
    "        data_List.append(data)\n",
    "\n",
    "    more_than_80_percent_similar = 0\n",
    "    between_40_and_80_percent_similar = 0\n",
    "    less_than_40_percent_similar = 0\n",
    "\n",
    "    for item in data_List:\n",
    "        str1, str2 = str(item[\"street1\"]), str(item[\"street2\"])\n",
    "        similarity = ratio(str1.lower(), str2.lower())  # Similarity as a number between 0 and 1\n",
    "        if similarity > 0.8:\n",
    "            more_than_80_percent_similar += 1\n",
    "        elif similarity >= 0.4 and similarity <= 0.8:\n",
    "            between_40_and_80_percent_similar += 1\n",
    "        elif similarity < 0.4:\n",
    "            less_than_40_percent_similar += 1\n",
    "\n",
    "    print(f\"\"\"\n",
    "{more_than_80_percent_similar} duplicates based on apartmentID, where the street names are more than 80% similar. --> These are probably the same street with different spellings.\n",
    "{between_40_and_80_percent_similar} duplicates based on apartmentID, where the street names are between 40% and 80% similar. --> These are probably the same street with minor differences.\n",
    "{less_than_40_percent_similar} duplicates based on apartmentID, where the street names are less than 40% similar. --> These are probably different streets.\"\"\")\n",
    "\n",
    "    return less_than_40_percent_similar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "14 duplicates based on apartmentID, where the street names are more than 80% similar. --> These are probably the same street with different spellings.\n",
      "3 duplicates based on apartmentID, where the street names are between 40% and 80% similar. --> These are probably the same street with minor differences.\n",
      "1 duplicates based on apartmentID, where the street names are less than 40% similar. --> These are probably different streets.\n"
     ]
    }
   ],
   "source": [
    "less_than_40_percent_similar = compare_streetnames(df_FalseAddress)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "The percentage of duplicates based on apartment ID, where  0.0736 % of the street names are less than 40% similar.\n",
      "This percentage means that there may be ads that are identical in terms of apartmentID, but have different addresses and are therefore not duplicates.\n",
      "Is this percentage acceptable? If it is less than 1%: True\n",
      "\n",
      "The percentage is acceptable. The duplicates based on apartmentID with different addresses will be removed from the dataset.\n"
     ]
    }
   ],
   "source": [
    "print(f\"\"\" \n",
    "The percentage of duplicates based on apartment ID, where  {((less_than_40_percent_similar / len(df_comparison)) * 100):.4f} % of the street names are less than 40% similar.\n",
    "This percentage means that there may be ads that are identical in terms of apartmentID, but have different addresses and are therefore not duplicates.\n",
    "Is this percentage acceptable? If it is less than 1%: {1 > ((less_than_40_percent_similar / len(df_comparison)) * 100)}\n",
    "\"\"\")\n",
    "\n",
    "if 1 > ((less_than_40_percent_similar / len(df_comparison)) * 100):\n",
    "    print(\"The percentage is acceptable. The duplicates based on apartmentID with different addresses will be removed from the dataset.\")\n",
    "\n",
    "assert 1 > ((less_than_40_percent_similar / len(df_comparison)) * 100), \"The percentage is not acceptable. The duplicates based on apartmentID with different addresses will not be removed from the dataset.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1.2.3: View Same IDs with False Title and Address"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "30 duplicates based on apartmentID, where the street names are more than 80% similar. --> These are probably the same street with different spellings.\n",
      "8 duplicates based on apartmentID, where the street names are between 40% and 80% similar. --> These are probably the same street with minor differences.\n",
      "4 duplicates based on apartmentID, where the street names are less than 40% similar. --> These are probably different streets.\n"
     ]
    }
   ],
   "source": [
    "df_AllDifferent = df_comparison[(df_comparison['same_id'] == True) & (df_comparison['same_title'] == False) & (df_comparison['same_address'] == False)]\n",
    "\n",
    "less_than_40_percent_similar = compare_streetnames(df_AllDifferent)\n",
    "\n",
    "#display_as_table(df_AllDifferent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "The percentage of duplicates based on apartment ID, where  0.2946 % of the street names are less than 40% similar.\n",
      "This percentage means that there may be ads that are identical in terms of apartmentID, but have different addresses and are therefore not duplicates.\n",
      "Is this percentage acceptable? If it is less than 1%: True\n"
     ]
    }
   ],
   "source": [
    "print(f\"\"\" \n",
    "The percentage of duplicates based on apartment ID, where  {((less_than_40_percent_similar / len(df_comparison)) * 100):.4f} % of the street names are less than 40% similar.\n",
    "This percentage means that there may be ads that are identical in terms of apartmentID, but have different addresses and are therefore not duplicates.\n",
    "Is this percentage acceptable? If it is less than 1%: {1 > ((less_than_40_percent_similar / len(df_comparison)) * 100)}\"\"\")\n",
    "\n",
    "assert 1 > ((less_than_40_percent_similar / len(df_comparison)) * 100), \"The percentage of duplicates based on apartment ID, where the street names are less than 40% similar is too high. Please check the data.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1069 duplicates based on apartmentID, that have the same titles and the same address.\n",
      "      \n",
      "These are the exact duplicates and need to be removed.\n"
     ]
    }
   ],
   "source": [
    "df_AllSame = df_comparison[(df_comparison['same_id'] == True) & (df_comparison['same_title'] == True) & (df_comparison['same_address'] == True)]\n",
    "\n",
    "print(f\"\"\"\n",
    "{len(df_AllSame)} duplicates based on apartmentID, that have the same titles and the same address.\n",
    "      \n",
    "These are the exact duplicates and need to be removed.\"\"\")\n",
    "\n",
    "#display_as_table(df_falseadress)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(df_comparison) == (len(df_FalseTitle) + len(df_FalseAddress) + len(df_AllSame) + len(df_AllDifferent)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 1.3: Filter duplicate IDs from the WG-Gesucht Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop_duplicates(subset='apartmentID')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_parquet(os.path.join(parent_dir, 'DataAnalysis/Data/apartmentsBerlinDataCleaned.parquet'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
