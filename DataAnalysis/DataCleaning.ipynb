{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WG - Gesucht Data Analysis \n",
    "#### Step 1: Data Cleaning \n",
    "\n",
    "- Data cleaning involves filtering out premium status ads and removing duplicates. \n",
    "- Duplicates are identified by the combination of title, address, and duplicate IDs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The last changed date of the CSV file is: 24.03.2024\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The length of the entire dataset is: 16120\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "import os\n",
    "import datetime\n",
    "\n",
    "# Get the parent directory of the current working directory\n",
    "parent_dir = os.path.abspath(os.path.join(os.getcwd(), os.pardir))\n",
    "\n",
    "apartmentsDataPath = os.path.join(parent_dir, 'WebCrawlerApp/data/output/apartmentsBerlinData.csv')\n",
    "\n",
    "# Get the last modified timestamp of the CSV file\n",
    "last_modified = os.path.getmtime(apartmentsDataPath)\n",
    "\n",
    "# Convert the timestamp to a datetime object\n",
    "last_modified_date = datetime.datetime.fromtimestamp(last_modified)\n",
    "\n",
    "# Print the last modified date\n",
    "print(f\"The last changed date of the CSV file is: {last_modified_date.strftime('%d.%m.%Y')}\")\n",
    "\n",
    "# Read the CSV file\n",
    "df = pd.read_csv(apartmentsDataPath, usecols=list(range(0, 45)))\n",
    "\n",
    "print(f\"The length of the entire dataset is: {len(df)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 1.1: Filter Premium Ads from the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The size of the dataset reduced by Premium ads is: 10233 \n",
      "\n",
      "There are 5887 Premium ads in the dataset.\n"
     ]
    }
   ],
   "source": [
    "df_without_premium = df[df['premiumstatus'] == False]\n",
    "\n",
    "print(f\"The size of the dataset reduced by Premium ads is: {len(df_without_premium)} \\n\\nThere are {len(df) - len(df_without_premium)} Premium ads in the dataset.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 1.2: Filter duplicate IDs from the WG-Gesucht Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Overwrite the original DataFrame with the one without Premium ads\n",
    "df = df_without_premium\n",
    "\n",
    "def compare_duplicates (column_name: str): \n",
    "    # Step 1: Find duplicates based on apartmentID\n",
    "    dup_ids = df[df.duplicated(subset=[column_name], keep=False)][column_name].unique()\n",
    "\n",
    "    # Prepare a new DataFrame for the comparison\n",
    "    comparison_columns = ['apartmentID1', 'apartmentID2', 'same_id', 'title1', 'title2', 'same_title', 'address1', 'address2', 'same_address', 'street1', 'street2', 'same_street', 'zip1', 'zip2', 'same_zip', 'city1', 'city2', 'same_city']\n",
    "    df_comparison = pd.DataFrame(columns=comparison_columns)\n",
    "\n",
    "    # Step 2: Find duplicates based on ID\n",
    "    for apt_id in dup_ids:\n",
    "        # Filter the DataFrame for the current apartmentID\n",
    "        dup_df = df[df[column_name] == apt_id]\n",
    "\n",
    "        # Ensure there are at least two entries for comparison\n",
    "        if len(dup_df) < 2:\n",
    "            print(dup_df)\n",
    "            continue  \n",
    "        \n",
    "        # Choose the first two entries for comparison\n",
    "        first, second = dup_df.iloc[0], dup_df.iloc[1]\n",
    "        \n",
    "        # Create a new row for the comparison DataFrame\n",
    "        new_row = {\n",
    "            'apartmentID1': first['apartmentID'], \n",
    "            'apartmentID2': second['apartmentID'], \n",
    "            'same_id': True,  # Da sie dieselbe apartmentID haben\n",
    "            'title1': first['title'], \n",
    "            'title2': second['title'], \n",
    "            'same_title': first['title'] == second['title'], \n",
    "            'address1': first['address'], \n",
    "            'address2': second['address'], \n",
    "            'same_address': first['address'] == second['address'], \n",
    "            'street1': first['street'],\n",
    "            'street2': second['street'],\n",
    "            'same_street': first['street'] == second['street'],\n",
    "            'zip1': first['postcode'],\n",
    "            'zip2': second['postcode'],\n",
    "            'same_zip': first['postcode'] == second['postcode'],\n",
    "            'city1': first['city'],\n",
    "            'city2': second['city'],\n",
    "            'same_city': first['city'] == second['city']\n",
    "        }\n",
    "        df_comparison = pd.concat([df_comparison, pd.DataFrame([new_row], columns=comparison_columns)])\n",
    "\n",
    "    print(f\"Number of duplicates based on {column_name}: {len(df_comparison)}\")\n",
    "    return df_comparison\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of duplicates based on apartmentID: 1358\n"
     ]
    }
   ],
   "source": [
    "df_comparison = compare_duplicates('apartmentID')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1.2.1: View Same IDs and address with different titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "229 duplicates based on apartmentID, that have the same address but different titles.\n",
      "\n",
      "When you view the table, you can see that the user or owner of the add changed the title over time. \n",
      "\n",
      "Result: Duplicates based on ID and Address with false Title will be removed from the dataset. \n"
     ]
    }
   ],
   "source": [
    "df_FalseTitle = df_comparison[(df_comparison['same_id'] == True) & (df_comparison['same_title'] == False) & (df_comparison['same_address'] == True)]\n",
    "\n",
    "print(f\"\"\"\n",
    "{len(df_FalseTitle)} duplicates based on apartmentID, that have the same address but different titles.\n",
    "\n",
    "When you view the table, you can see that the user or owner of the add changed the title over time. \n",
    "\n",
    "Result: Duplicates based on ID and Address with false Title will be removed from the dataset. \"\"\")\n",
    "\n",
    "#display_as_table(df_FalseTitle) # Here you can output the entire table of the False titles with duplicate ids and addresses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1.2.2: View Same IDs and Title with different address. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "18 duplicates based on apartmentID, that have the same titles but with different addresses.\n"
     ]
    }
   ],
   "source": [
    "df_FalseAddress = df_comparison[(df_comparison['same_id'] == True) & (df_comparison['same_title'] == True) & (df_comparison['same_address'] == False) ]\n",
    "\n",
    "print(f\"\"\"\n",
    "{len(df_FalseAddress)} duplicates based on apartmentID, that have the same titles but with different addresses.\"\"\")\n",
    "\n",
    "#display_as_table(df_falseadressAndTitle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Levenshtein import ratio\n",
    "\n",
    "def compare_streetnames(df_differentstreets):\n",
    "    data_List = []\n",
    "\n",
    "    for index, row in df_differentstreets.iterrows():\n",
    "        data = {'street1' : row['street1'], 'street2' : row['street2']}\n",
    "        data_List.append(data)\n",
    "\n",
    "    more_than_80_percent_similar = 0\n",
    "    between_40_and_80_percent_similar = 0\n",
    "    less_than_40_percent_similar = 0\n",
    "\n",
    "    for item in data_List:\n",
    "        str1, str2 = str(item[\"street1\"]), str(item[\"street2\"])\n",
    "        similarity = ratio(str1.lower(), str2.lower())  # Similarity as a number between 0 and 1\n",
    "        if similarity > 0.8:\n",
    "            more_than_80_percent_similar += 1\n",
    "        elif similarity >= 0.4 and similarity <= 0.8:\n",
    "            between_40_and_80_percent_similar += 1\n",
    "        elif similarity < 0.4:\n",
    "            less_than_40_percent_similar += 1\n",
    "\n",
    "    print(f\"\"\"\n",
    "{more_than_80_percent_similar} duplicates based on apartmentID, where the street names are more than 80% similar. --> These are probably the same street with different spellings.\n",
    "{between_40_and_80_percent_similar} duplicates based on apartmentID, where the street names are between 40% and 80% similar. --> These are probably the same street with minor differences.\n",
    "{less_than_40_percent_similar} duplicates based on apartmentID, where the street names are less than 40% similar. --> These are probably different streets.\"\"\")\n",
    "\n",
    "    return less_than_40_percent_similar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "14 duplicates based on apartmentID, where the street names are more than 80% similar. --> These are probably the same street with different spellings.\n",
      "3 duplicates based on apartmentID, where the street names are between 40% and 80% similar. --> These are probably the same street with minor differences.\n",
      "1 duplicates based on apartmentID, where the street names are less than 40% similar. --> These are probably different streets.\n"
     ]
    }
   ],
   "source": [
    "less_than_40_percent_similar = compare_streetnames(df_FalseAddress)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "The percentage of duplicates based on apartment ID, where  0.0736 % of the street names are less than 40% similar.\n",
      "This percentage means that there may be ads that are identical in terms of apartmentID, but have different addresses and are therefore not duplicates.\n",
      "Is this percentage acceptable? If it is less than 1%: True\n",
      "\n",
      "The percentage is acceptable. The duplicates based on apartmentID with different addresses will be removed from the dataset.\n"
     ]
    }
   ],
   "source": [
    "print(f\"\"\" \n",
    "The percentage of duplicates based on apartment ID, where  {((less_than_40_percent_similar / len(df_comparison)) * 100):.4f} % of the street names are less than 40% similar.\n",
    "This percentage means that there may be ads that are identical in terms of apartmentID, but have different addresses and are therefore not duplicates.\n",
    "Is this percentage acceptable? If it is less than 1%: {1 > ((less_than_40_percent_similar / len(df_comparison)) * 100)}\n",
    "\"\"\")\n",
    "\n",
    "if 1 > ((less_than_40_percent_similar / len(df_comparison)) * 100):\n",
    "    print(\"The percentage is acceptable. The duplicates based on apartmentID with different addresses will be removed from the dataset.\")\n",
    "\n",
    "assert 1 > ((less_than_40_percent_similar / len(df_comparison)) * 100), \"The percentage is not acceptable. The duplicates based on apartmentID with different addresses will not be removed from the dataset.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1.2.3: View Same IDs with False Title and Address"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "30 duplicates based on apartmentID, where the street names are more than 80% similar. --> These are probably the same street with different spellings.\n",
      "8 duplicates based on apartmentID, where the street names are between 40% and 80% similar. --> These are probably the same street with minor differences.\n",
      "4 duplicates based on apartmentID, where the street names are less than 40% similar. --> These are probably different streets.\n"
     ]
    }
   ],
   "source": [
    "df_AllDifferent = df_comparison[(df_comparison['same_id'] == True) & (df_comparison['same_title'] == False) & (df_comparison['same_address'] == False)]\n",
    "\n",
    "less_than_40_percent_similar = compare_streetnames(df_AllDifferent)\n",
    "\n",
    "#display_as_table(df_AllDifferent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "The percentage of duplicates based on apartment ID, where  0.2946 % of the street names are less than 40% similar.\n",
      "This percentage means that there may be ads that are identical in terms of apartmentID, but have different addresses and are therefore not duplicates.\n",
      "Is this percentage acceptable? If it is less than 1%: True\n"
     ]
    }
   ],
   "source": [
    "print(f\"\"\" \n",
    "The percentage of duplicates based on apartment ID, where  {((less_than_40_percent_similar / len(df_comparison)) * 100):.4f} % of the street names are less than 40% similar.\n",
    "This percentage means that there may be ads that are identical in terms of apartmentID, but have different addresses and are therefore not duplicates.\n",
    "Is this percentage acceptable? If it is less than 1%: {1 > ((less_than_40_percent_similar / len(df_comparison)) * 100)}\"\"\")\n",
    "\n",
    "assert 1 > ((less_than_40_percent_similar / len(df_comparison)) * 100), \"The percentage of duplicates based on apartment ID, where the street names are less than 40% similar is too high. Please check the data.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1069 duplicates based on apartmentID, that have the same titles and the same address.\n",
      "      \n",
      "These are the exact duplicates and need to be removed.\n"
     ]
    }
   ],
   "source": [
    "df_AllSame = df_comparison[(df_comparison['same_id'] == True) & (df_comparison['same_title'] == True) & (df_comparison['same_address'] == True)]\n",
    "\n",
    "print(f\"\"\"\n",
    "{len(df_AllSame)} duplicates based on apartmentID, that have the same titles and the same address.\n",
    "      \n",
    "These are the exact duplicates and need to be removed.\"\"\")\n",
    "\n",
    "#display_as_table(df_falseadress)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(df_comparison) == (len(df_FalseTitle) + len(df_FalseAddress) + len(df_AllSame) + len(df_AllDifferent)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 1.3: Filter duplicates, non values, 1-room-apartments from the WG-Gesucht Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The length of the dataset after removing duplicates is: 8875\n",
      "The length of the dataset after removing NaN for roomsize is: 8861\n",
      "The length of the dataset after removing NaN for suburb is: 8717\n",
      "The length of the dataset after removing duplicates based on title and address is: 8548\n",
      "The length of the dataset after removing ads with user_name is: 8363\n"
     ]
    }
   ],
   "source": [
    "df = df.drop_duplicates(subset='apartmentID')\n",
    "\n",
    "print(f\"The length of the dataset after removing duplicates is: {len(df)}\")\n",
    "\n",
    "df = df.dropna(subset=['room_size'])\n",
    "df = df[df['room_size'] != 'n.a.']\n",
    "\n",
    "print(f\"The length of the dataset after removing NaN for roomsize is: {len(df)}\")\n",
    "\n",
    "df = df.dropna(subset=['total_rent'])\n",
    "\n",
    "df = df.dropna(subset=['suburb'])\n",
    "\n",
    "print(f\"The length of the dataset after removing NaN for suburb is: {len(df)}\")\n",
    "\n",
    "df = df.drop_duplicates(subset=['title', 'address', 'rent'])\n",
    "\n",
    "print(f\"The length of the dataset after removing duplicates based on title and address is: {len(df)}\")\n",
    "\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "df_notnull = df[df['user_name'].notnull()]\n",
    "\n",
    "df = df[df['user_name'].isnull()]\n",
    "\n",
    "print(f\"The length of the dataset after removing ads with user_name is: {len(df)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The length of the dataset after removing 1-room apartments is: 8116\n"
     ]
    }
   ],
   "source": [
    "pattern = r'1-Zimmer-Wohnung|1-Zimmer Wohnung|1-room Apartment|1-room-apartment|1-room-studio'\n",
    "\n",
    "df = df[~df['title'].str.contains(pattern, case=False, na=False)]\n",
    "\n",
    "print(f\"The length of the dataset after removing 1-room apartments is: {len(df)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 1.4: Update total rent for short term contracts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean the data and convert the columns to the right data type for calculations\n",
    "df.loc[:, 'room_size'] = df['room_size'].str.replace('m²', '').astype(float)\n",
    "df.loc[:, 'apartment_size'] = df['apartment_size'].str.replace('m²', '').astype(float)\n",
    "df.loc[:, 'total_rent'] = df['total_rent'].str.replace('€', '').astype(float)\n",
    "df.loc[:, 'max_roommate'] = df['max_roommate'].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to adjust rent based on availability duration\n",
    "def adjust_rent(df):\n",
    "    df_duration = pd.DataFrame()\n",
    "    df_duration.loc[:, 'available_from'] = pd.to_datetime(df['available_from'], dayfirst=True)\n",
    "    df_duration.loc[:, 'available_until'] = pd.to_datetime(df['available_until'], dayfirst=True)\n",
    "\n",
    "    # Calculating the adjusted rent\n",
    "    for index, row in df_duration.iterrows():\n",
    "        if pd.notna(row['available_until']):\n",
    "            diff_days = (row['available_until'] - row['available_from']).days\n",
    "            if diff_days < 30:\n",
    "             df.at[index, 'rent_updated'] = (df.at[index, 'total_rent'] / diff_days) * 30\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inititalizing new column updated rent\n",
    "df['rent_updated'] = df['total_rent'].copy()\n",
    "\n",
    "df = adjust_rent(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 1.5: Clean the suburb columns for price analysis and add districts "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "suburb_to_district_path = os.path.join(parent_dir, 'WebCrawlerApp/data/input/relationSuburbsToDistrict.xlsx')\n",
    "\n",
    "xlsx = pd.ExcelFile(suburb_to_district_path)\n",
    "\n",
    "suburbs_to_district_dict = {}\n",
    "\n",
    "for sheet_name in xlsx.sheet_names:\n",
    "    suburbs_to_district_dict[sheet_name] = xlsx.parse(sheet_name).iloc[:, 0].dropna().tolist()\n",
    "\n",
    "def find_suburb_and_district(address):\n",
    "    # Regex, um zu überprüfen, ob der Vorort-/Bezirksname von Leerzeichen oder Satzzeichen umgeben ist\n",
    "    address = address.lower()\n",
    "    for district, suburbs in suburbs_to_district_dict.items():\n",
    "        for suburb in suburbs:\n",
    "            # Suchmuster, das sicherstellt, dass der Name als ganzes Wort erscheint\n",
    "            pattern = r'\\b' + re.escape(suburb.lower()) + r'\\b'\n",
    "            if re.search(pattern, address):\n",
    "                return suburb, district\n",
    "    return None, None\n",
    "\n",
    "df['suburb'], df['district'] = zip(*df['address'].map(find_suburb_and_district))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reorder the columns in the DataFrame\n",
    "columns = list(df.columns)\n",
    "total_rent_index = columns.index('total_rent')\n",
    "suburb_index = columns.index('suburb')\n",
    "columns_to_move = ['rent_updated', 'district']\n",
    "\n",
    "columns.remove('rent_updated')\n",
    "columns.remove('district')\n",
    "\n",
    "columns.insert(total_rent_index + 1, 'rent_updated')\n",
    "columns.insert(suburb_index + 1, 'district')\n",
    "\n",
    "# Reorder the DataFrame according to the new column order\n",
    "df = df[columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_parquet(os.path.join(parent_dir, 'DataAnalysis/Data/apartmentsBerlinDataCleaned.parquet'))\n",
    "df.to_csv(os.path.join(parent_dir, 'DataAnalysis/Data/apartmentsBerlinDataCleaned.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The cleaned dataset has the length of 8116 and is saved as a parquet file.\n"
     ]
    }
   ],
   "source": [
    "print(f'The cleaned dataset has the length of {len(df)} and is saved as a parquet file.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
